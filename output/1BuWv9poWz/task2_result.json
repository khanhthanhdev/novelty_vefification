{
  "mode": "per_contribution",
  "paper_year": null,
  "queries": [
    {
      "id": "C1",
      "query": "Enhancing transferable adversarial attacks Vision Transformers Gradient Normalization Scaling",
      "status": "ok",
      "count": 10
    },
    {
      "id": "C2",
      "query": "Enhancing transferable adversarial attacks Vision Transformers High-Frequency Adaptation",
      "status": "ok",
      "count": 10
    }
  ],
  "candidate_pool_top30": [
    {
      "cand_id": "b57885276acc5c4f893f7a648c1fbb6f0a9ff7dc",
      "title": "Enhancing Transferable Adversarial Attacks on Vision Transformers through Gradient Normalization Scaling and High-Frequency Adaptation",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "abstract": "",
      "url": "https://www.semanticscholar.org/paper/b57885276acc5c4f893f7a648c1fbb6f0a9ff7dc",
      "embedding": null
    },
    {
      "cand_id": "b7c957e591559eea51c3f23247de5fd84ca9c0ce",
      "title": "TESSER: Transfer-Enhancing Adversarial Attacks from Vision Transformers via Spectral and Semantic Regularization",
      "year": 2025,
      "venue": "arXiv.org",
      "abstract": "Adversarial transferability remains a critical challenge in evaluating the robustness of deep neural networks. In security-critical applications, transferability enables black-box attacks without access to model internals, making it a key concern for real-world adversarial threat assessment. While Vision Transformers (ViTs) have demonstrated strong adversarial performance, existing attacks often fail to transfer effectively across architectures, especially from ViTs to Convolutional Neural Networks (CNNs) or hybrid models. In this paper, we introduce \\textbf{TESSER} -- a novel adversarial attack framework that enhances transferability via two key strategies: (1) \\textit{Feature-Sensitive Gradient Scaling (FSGS)}, which modulates gradients based on token-wise importance derived from intermediate feature activations, and (2) \\textit{Spectral Smoothness Regularization (SSR)}, which suppresses high-frequency noise in perturbations using a differentiable Gaussian prior. These components work in tandem to generate perturbations that are both semantically meaningful and spectrally smooth. Extensive experiments on ImageNet across 12 diverse architectures demonstrate that TESSER achieves +10.9\\% higher attack succes rate (ASR) on CNNs and +7.2\\% on ViTs compared to the state-of-the-art Adaptive Token Tuning (ATT) method. Moreover, TESSER significantly improves robustness against defended models, achieving 53.55\\% ASR on adversarially trained CNNs. Qualitative analysis shows strong alignment between TESSER's perturbations and salient visual regions identified via Grad-CAM, while frequency-domain analysis reveals a 12\\% reduction in high-frequency energy, confirming the effectiveness of spectral regularization.",
      "url": "https://www.semanticscholar.org/paper/b7c957e591559eea51c3f23247de5fd84ca9c0ce",
      "embedding": null
    },
    {
      "cand_id": "906917938fc1d1dd1f5bb583ec6cd33817370521",
      "title": "Robust Token Gradient and Frequency-Aware Transferable Adversarial Attacks on Vision Transformers",
      "year": 2025,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "abstract": "Vision Transformers (ViTs) have achieved remarkable performance in computer vision tasks but are vulnerable to adversarial attacks. Recent studies have demonstrated the feasibility of crafting transferable adversarial examples based on ViT models. However, the adversarial examples generated by ViTs exhibit poor generalization, primarily due to structural differences between models and the tendency to overfit, which significantly hinders cross-architecture transferability. In this paper, we propose a novel framework to improve the generalization and transferability of adversarial attacks across diverse models, focusing on two key strategies: Token Gradient Divergence (TGD) and Multi-level Frequency-aware Attack (MFA). TGD, as a gradient regularization method, addresses the structural gradient issue of surrogate models, which is one of the causes of overfitting. By increasing the gradient divergence between tokens and eliminating the influence of the class token gradient, TGD enhances the transferability of adversarial examples across models. Meanwhile, MFA employs an implicit ensemble approach to enhance attack generalization. Through multiple spectral augmentations, it increases input diversity and simulates ensemble learning. By targeting critical frequency regions across models, MFA enhances adversarial example adaptability to different architectures, significantly boosting cross-architecture transferability. Extensive experiments on both ViTs and CNNs demonstrate that TGD-MFA significantly outperforms state-of-the-art transfer-based attacks, achieving substantial improvements in adversarial transferability and robustness.",
      "url": "https://www.semanticscholar.org/paper/906917938fc1d1dd1f5bb583ec6cd33817370521",
      "embedding": null
    },
    {
      "cand_id": "31109f3be997fe6455709bd0e69d5722e0324c82",
      "title": "Transferable Adversarial Attacks on Transformer and CNN",
      "year": 2023,
      "venue": "ACM Cloud and Autonomic Computing Conference",
      "abstract": "Vision Transformers (ViTs) are attention-based encoder-decoder architectures widely used in computer vision (CV) and have shown excellent performance. However, ViTs are also vulnerable to adversarial attacks. In actuality, the adversarial samples produced by various models exhibit some degree of overfitting. The primary challenge with anti-sample crossmodel transfer is based on this. In this research, a novel approach to enhance the transferability of adversarial samples between various ViTs and between ViTs and CNNs is proposed. The main steps of this method include: dividing the original image (224*224) into patches (16*16), extracting high-frequency information and calculating interaction values, and then applying momentum attacks to reduce the interaction between patches. (The negative correlation between game interaction among adversarial perturbation units and adversarial transferability has been verified in the work of Xin Wang et al. [1].) According to experimental findings, the suggested technique may significantly enhance the transferability of ViTs and between ViTs and CNNs.",
      "url": "https://www.semanticscholar.org/paper/31109f3be997fe6455709bd0e69d5722e0324c82",
      "embedding": null
    },
    {
      "cand_id": "10d7d1a137cb573067dbd334e84e823566e6a345",
      "title": "ViT-EnsembleAttack: Augmenting Ensemble Models for Stronger Adversarial Transferability in Vision Transformers",
      "year": 2025,
      "venue": "arXiv.org",
      "abstract": "Ensemble-based attacks have been proven to be effective in enhancing adversarial transferability by aggregating the outputs of models with various architectures. However, existing research primarily focuses on refining ensemble weights or optimizing the ensemble path, overlooking the exploration of ensemble models to enhance the transferability of adversarial attacks. To address this gap, we propose applying adversarial augmentation to the surrogate models, aiming to boost overall generalization of ensemble models and reduce the risk of adversarial overfitting. Meanwhile, observing that ensemble Vision Transformers (ViTs) gain less attention, we propose ViT-EnsembleAttack based on the idea of model adversarial augmentation, the first ensemble-based attack method tailored for ViTs to the best of our knowledge. Our approach generates augmented models for each surrogate ViT using three strategies: Multi-head dropping, Attention score scaling, and MLP feature mixing, with the associated parameters optimized by Bayesian optimization. These adversarially augmented models are ensembled to generate adversarial examples. Furthermore, we introduce Automatic Reweighting and Step Size Enlargement modules to boost transferability. Extensive experiments demonstrate that ViT-EnsembleAttack significantly enhances the adversarial transferability of ensemble-based attacks on ViTs, outperforming existing methods by a substantial margin. Code is available at https://github.com/Trustworthy-AI-Group/TransferAttack.",
      "url": "https://www.semanticscholar.org/paper/10d7d1a137cb573067dbd334e84e823566e6a345",
      "embedding": null
    },
    {
      "cand_id": "3bfe2a47bbe73d0fd19eb1d741fb429db897f9de",
      "title": "Three dimensional segmentation of abdominal arteries and veins using vision transformers and domain adaptation",
      "year": 2025,
      "venue": "Physics in Medicine and Biology",
      "abstract": "Accurate segmentation of abdominal three-dimensional (3D) vascular structures from computed tomography (CT) scans is crucial for clinical applications yet remains challenging due to dependency on large annotated datasets through effective pretraining and poor generalization via cross-domain feature alignment. In this paper, we proposes a novel transformer-based framework based on masked autoencoder (MAE) and UNEt TRansformers (UNETR), dubbed as Adaptive MAE-UNETR, that integrates self-supervised pretraining and adversarial domain adaptation to achieve robust artery/vein segmentation with enhanced generalization. Specifically, first, we develop a MAE pretraining paradigm with 3D CT scans as input, to learn hierarchical feature representations through self-reconstruction tasks from source domain data. This targeted design ensures high fidelity in feature extraction and improves segmentation accuracy and stability. Second, the pretrained encoder is transferred to a UNETR segmentation network augmented with domain adaptation technique, which adversarially aligns feature distributions between source and target domains via a domain discriminator. Third, we establish an segmentation framework that simultaneously optimizes segmentation accuracy and domain invariance. Comprehensive evaluations on three public datasets demonstrate state-of-the-art performance of our proposed method. On AMOS22 dataset, our model achieves DSC scores of 0.924 (aorta) and 0.892 (inferior vena cava, IVC). Cross-domain tests yield 0.917/0.888 on BTCV dataset and 0.931/0.906 on FLARE23 dataset, showing consistent superiority over conventional methods across diverse datasets.",
      "url": "https://www.semanticscholar.org/paper/3bfe2a47bbe73d0fd19eb1d741fb429db897f9de",
      "embedding": null
    },
    {
      "cand_id": "017883fd7fc01dd11cf5dc0e13d68eaf0f4f5619",
      "title": "MAS-PD: Transferable Adversarial Attack Against Vision-Transformers-Based SAR Image Classification Task",
      "year": 2025,
      "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",
      "abstract": "Synthetic aperture radar (SAR) is widely used in civil and military fields. With advancements in vision transformer (ViT) research, these models have become increasingly important in SAR image classification due to their remarkable performance. Therefore, effectively interfering with the classification results of enemy radar systems has become a crucial factor in ensuring battlefield security. Adversarial attacks offer a potential solution, as they can significantly mislead models and cause incorrect predictions. However, recent research on adversarial examples focus on the vulnerability of convolutional neural network (CNN) models, while the attack on transformer models has not been extensively studied. Considering that ViTs differ from CNNs due to its unique multihead self-attention (MSA) mechanism and its approach of segmenting images into patches for input, this article proposes a “MAS-PD” black-box adversarial attack method targeting these two mechanisms in ViTs. First, to target the MSA mechanism, we propose the Momentum Attention Skipping attack. By skipping the attention gradient during backpropagation and using momentum to avoid local maxima during gradient ascent, our method enhances the transferability of adversarial attacks across different models. Then, we apply dropout on input patches in each iteration, achieving higher attack success rates compared to using all patches. We compare our method with four traditional adversarial attack techniques across different model architectures, including CNNs and ViTs, using the publicly available MSTAR SAR dataset. The experimental results show that our method achieves an average Attack Success Rate (ASR) of 68.82% across ViTs, while other methods achieve no more than 50% ASR on average. When applied to CNNs, our method also achieves an average ASR of 67.14%, compared to less than 40% ASR for other methods. The experiment results demonstrate that our algorithm significantly enhances transferability between ViTs and from ViTs to CNNs in SAR image classification tasks.",
      "url": "https://www.semanticscholar.org/paper/017883fd7fc01dd11cf5dc0e13d68eaf0f4f5619",
      "embedding": null
    },
    {
      "cand_id": "de0454637d24b02fb57bc4ac863664dfa2c91d44",
      "title": "Improving Vision Transformers by Revisiting High-frequency Components",
      "year": 2022,
      "venue": "European Conference on Computer Vision",
      "abstract": "The transformer models have shown promising effectiveness in dealing with various vision tasks. However, compared with training Convolutional Neural Network (CNN) models, training Vision Transformer (ViT) models is more difficult and relies on the large-scale training set. To explain this observation we make a hypothesis that \\textit{ViT models are less effective in capturing the high-frequency components of images than CNN models}, and verify it by a frequency analysis. Inspired by this finding, we first investigate the effects of existing techniques for improving ViT models from a new frequency perspective, and find that the success of some techniques (e.g., RandAugment) can be attributed to the better usage of the high-frequency components. Then, to compensate for this insufficient ability of ViT models, we propose HAT, which directly augments high-frequency components of images via adversarial training. We show that HAT can consistently boost the performance of various ViT models (e.g., +1.2% for ViT-B, +0.5% for Swin-B), and especially enhance the advanced model VOLO-D5 to 87.3% that only uses ImageNet-1K data, and the superiority can also be maintained on out-of-distribution data and transferred to downstream tasks. The code is available at: https://github.com/jiawangbai/HAT.",
      "url": "https://www.semanticscholar.org/paper/de0454637d24b02fb57bc4ac863664dfa2c91d44",
      "embedding": null
    },
    {
      "cand_id": "1cf9fdb895dcc71647cfc98cc55e6ead3112aef2",
      "title": "Enhancing Adversarial Attacks With Two‐Way Gradient Adjustment and Neighborhood Resampling",
      "year": 2026,
      "venue": "International Conference on Climate Informatics",
      "abstract": "\n \n Deep neural networks (DNNs) are highly susceptible to adversarial examples. By adding imperceptible perturbations to benign images, adversarial examples can mislead models into producing incorrect outputs, thereby posing significant security risks to DNN‐based applications. However, most existing adversarial attack methods still achieve limited attack success rates. Focusing on black‐box transfer attacks, we propose two novel approaches. First, TNI‐FGSM aggregates gradients from the forward, backward, and current directions, enabling more stable updates and yielding more optimal perturbation directions. Second, NRSM initially performs random sampling on the input, followed by neighborhood resampling on both sides of the previous iteration's gradient. This process captures richer information, facilitates the discovery of optimal local extrema, and enhances transferability. Experiments conducted on ImageNet across conventional CNNs, four types of vision Transformers, and robust models demonstrate that NRSM consistently outperforms baseline methods. When Inception‐v3 (Inc‐v3) is used as the local model, NRSM achieves attack success rates (ASRs) that are 11.9% and 15.4% higher than LETM on Swin and HGD, respectively. Under the ensemble setting, NRSM attains an average ASR of 96.5%, surpassing Admix by 6.1%. Code is available at\n https://github.com/BreenoWH/NRSM‐TNI\n .\n",
      "url": "https://www.semanticscholar.org/paper/1cf9fdb895dcc71647cfc98cc55e6ead3112aef2",
      "embedding": null
    },
    {
      "cand_id": "e63b3d71a32e76800f11df8fbf6e36330b115363",
      "title": "Computer-Generated Image Forensics Based on Vision Transformer with High-Frequency Feature Enhancement",
      "year": 2025,
      "venue": "European Signal Processing Conference",
      "abstract": "Distinguishing computer-generated (CG) images from photographic (PG) images is an important task in multimedia forensics. Many deep learning-based methods have recently been proposed for CG image forensics. However, the detection performances of these methods still need to be improved, especially in terms of robustness against post-processing operations, thus limiting their practical applicability. To tackle these issues, we leverage the Vision Transformer (ViT) model, which excels in capturing the global features of images, and design a High-Frequency Feature Enhancement (HFFE) module to exploit the discriminative frequency information between CG and PG images. In our experiments, we evaluate the performance under various commonly used post-processing operations. Moreover, we test the performance in the presence of adversarial attacks, which is a more challenging real-world case. The experimental results demonstrate that our method achieves superior detection accuracy and significantly better robustness against post-processing operations and adversarial attacks when compared with the state-of-the-art methods.",
      "url": "https://www.semanticscholar.org/paper/e63b3d71a32e76800f11df8fbf6e36330b115363",
      "embedding": null
    },
    {
      "cand_id": "0083b30fcbc2be659d5a331e66eeb1654c63bd46",
      "title": "Enhancing Adversarial Transferability on Vision Transformer by Permutation-Invariant Attacks",
      "year": 2024,
      "venue": "IEEE International Conference on Multimedia and Expo",
      "abstract": "Vision Transformers (ViTs) have demonstrated remarkable performance in computer vision. However, they are still susceptible to adversarial examples. In this paper, we propose a novel adversarial attack method tailored for ViTs, by leveraging the inherent permutation-invariant of ViTs to generate highly transferable adversarial examples. Specifically, we split the image into patches of different scales and permute the local patches to generate diverse inputs. By optimizing perturbations on the permuted image set, we can prevent the generated adversarial examples from overfitting to the surrogate model, thereby enhancing transferability. Extensive experiments conducted on ImageNet demonstrate that the permutation-invariant (PI) attack significantly improves transferability between ViTs and from ViTs to CNNs. PI is applicable to diverse ViTs and can seamlessly integrate with existing attack methods further enhancing transferability. Our approach surpasses state-of-the-art ensemble methods for input transformation and achieves a notable performance improvement of 11.9% on average.",
      "url": "https://www.semanticscholar.org/paper/0083b30fcbc2be659d5a331e66eeb1654c63bd46",
      "embedding": null
    },
    {
      "cand_id": "a96a467f5e6edafbc32e441ea1625d3269688f46",
      "title": "Enhancing the transferability of adversarial examples on vision transformers",
      "year": 2024,
      "venue": "J. Electronic Imaging",
      "abstract": "Abstract. The advancement of adversarial attack techniques, particularly against neural network architectures, is a crucial area of research in machine learning. Notably, the emergence of vision transformers (ViTs) as a dominant force in computer vision tasks has opened avenues for exploring their vulnerabilities. In this context, we introduce dual gradient optimization for adversarial transferability (DGO-AT), a comprehensive strategy designed to enhance the transferability of adversarial examples in ViTs. DGO-AT incorporates two innovative components: attention gradient smoothing (AGS) and multi-layer perceptron gradient random dropout (GRD-MLP). AGS targets the attention layers of ViTs to smooth gradients and reduce noise, focusing on global features for improved transferability. GRD-MLP, on the other hand, introduces stochasticity into MLP gradient updates, broadening the adversarial examples’ applicability. The synergy of these strategies in DGO-AT addresses the unique structural aspects of ViTs, leading to more effective and transferable adversarial attacks. Our comprehensive evaluations of a variety of ViT and CNN models, using the ImageNet dataset, demonstrate that DGO-AT significantly enhances the effectiveness and transferability of attacks, thereby contributing to the ongoing discourse on the adversarial robustness of advanced neural network models.",
      "url": "https://www.semanticscholar.org/paper/a96a467f5e6edafbc32e441ea1625d3269688f46",
      "embedding": null
    },
    {
      "cand_id": "f3dc271f3b1871adc676d29a769664e073a0847e",
      "title": "Enhancing Transferability of Adversarial Examples Through Mixed-Frequency Inputs",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "abstract": "Recent studies have shown that Deep Neural Networks (DNNs) are easily deceived by adversarial examples, revealing their serious vulnerability. Due to the transferability, adversarial examples can attack across multiple models with different architectures, called transfer-based black-box attacks. Input transformation is one of the most effective methods to improve adversarial transferability. In particular, the attacks fusing other categories of image information reveal the potential direction of adversarial attacks. However, the current techniques rely on input transformations in the spatial domain, which ignore the frequency information of the image and limit its transferability. To tackle this issue, we propose Mixed-Frequency Inputs (MFI) based on a frequency domain perspective. MFI alleviates the overfitting of adversarial examples to the source model by considering high-frequency components from various kinds of images in the process of calculating the gradient. By accumulating these high-frequency components, MFI acquires a more steady gradient direction in each iteration, leading to the discovery of better local maxima and enhancing transferability. Extensive experimental results on the ImageNet-compatible datasets demonstrate that MFI outperforms existing transform-based attacks with a clear margin on both Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), which proves MFI is more suitable for realistic black-box scenarios.",
      "url": "https://www.semanticscholar.org/paper/f3dc271f3b1871adc676d29a769664e073a0847e",
      "embedding": null
    },
    {
      "cand_id": "0def290ae38abb4a04e35e0bcdc86b71d237f494",
      "title": "On the Adversarial Robustness of Vision Transformers",
      "year": 2022,
      "venue": "Trans. Mach. Learn. Res.",
      "abstract": "Following the success in advancing natural language processing and understanding, transformers are expected to bring revolutionary changes to computer vision. This work provides a comprehensive study on the robustness of vision transformers (ViTs) against adversarial perturbations. Tested on various white-box and transfer attack settings, we find that ViTs possess better adversarial robustness when compared with MLP-Mixer and convolutional neural networks (CNNs) including ConvNeXt, and this observation also holds for certified robustness. Through frequency analysis and feature visualization, we summarize the following main observations contributing to the improved robustness of ViTs: 1) Features learned by ViTs contain less high-frequency patterns that have spurious correlation, which helps explain why ViTs are less sensitive to high-frequency perturbations than CNNs and MLP-Mixer, and there is a high correlation between how much the model learns high-frequency features and its robustness against different frequency-based perturbations. 2) Introducing convolutional or tokens-to-token blocks for learning high-frequency features in ViTs can improve classification accuracy but at the cost of adversarial robustness. 3) Modern CNN designs that borrow techniques from ViTs including activation function, layer norm, larger kernel size to imitate the global attention, and patchify the images as inputs, etc., could help bridge the performance gap between ViTs and CNNs not only in terms of performance, but also certified and empirical adversarial robustness. Moreover, we show adversarial training is also applicable to ViT for training robust models, and sharpness-aware minimization can also help improve robustness, while pre-training with clean images on larger datasets does not significantly improve adversarial robustness.",
      "url": "https://www.semanticscholar.org/paper/0def290ae38abb4a04e35e0bcdc86b71d237f494",
      "embedding": null
    },
    {
      "cand_id": "33bd32b655949162cadc307b6647ebf9a911e50a",
      "title": "Towards Transferable Adversarial Attacks on Image and Video Transformers",
      "year": 2023,
      "venue": "IEEE Transactions on Image Processing",
      "abstract": "The transferability of adversarial examples across different convolutional neural networks (CNNs) makes it feasible to perform black-box attacks, resulting in security threats for CNNs. However, fewer endeavors have been made to investigate transferable attacks for vision transformers (ViTs), which achieve superior performance on various computer vision tasks. Unlike CNNs, ViTs establish relationships between patches extracted from inputs by the self-attention module. Thus, adversarial examples crafted on CNNs might hardly attack ViTs. To assess the security of ViTs comprehensively, we investigate the transferability across different ViTs in both untargetd and targeted scenarios. More specifically, we propose a Pay No Attention (PNA) attack, which ignores attention gradients during backpropagation to improve the linearity of backpropagation. Additionally, we introduce a PatchOut/CubeOut attack for image/video ViTs. They optimize perturbations within a randomly selected subset of patches/cubes during each iteration, preventing over-fitting to the white-box surrogate ViT model. Furthermore, we maximize the $L_{2}$ norm of perturbations, ensuring that the generated adversarial examples deviate significantly from the benign ones. These strategies are designed to be harmoniously compatible. Combining them can enhance transferability by jointly considering patch-based inputs and the self-attention of ViTs. Moreover, the proposed combined attack seamlessly integrates with existing transferable attacks, providing an additional boost to transferability. We conduct experiments on ImageNet and Kinetics-400 for image and video ViTs, respectively. Experimental results demonstrate the effectiveness of the proposed method.",
      "url": "https://www.semanticscholar.org/paper/33bd32b655949162cadc307b6647ebf9a911e50a",
      "embedding": null
    },
    {
      "cand_id": "eba519c9e4e03fc4f52f3120a0d5803c62e36c4d",
      "title": "S-E Pipeline: A Vision Transformer (ViT) based Resilient Classification Pipeline for Medical Imaging Against Adversarial Attacks",
      "year": 2024,
      "venue": "IEEE International Joint Conference on Neural Network",
      "abstract": "Vision Transformer (ViT) is becoming widely popular in automating accurate disease diagnosis in medical imaging owing to its robust self-attention mechanism. However, ViTs remain vulnerable to adversarial attacks that may thwart the diagnosis process by leading it to intentional misclassification of critical disease. In this paper, we propose a novel image classification pipeline, namely, S-E Pipeline, that performs multiple pre-processing steps that allow ViT to be trained on critical features so as to reduce the impact of input perturbations by adversaries. Our method uses a combination of segmentation and image enhancement techniques such as Contrast Limited Adaptive Histogram Equalization (CLAHE), Unsharp Masking (UM), and High-Frequency Emphasis filtering (HFE) as preprocessing steps to identify critical features that remain intact even after adversarial perturbations. The experimental study demonstrates that our novel pipeline helps in reducing the effect of adversarial attacks by 72.22% for the ViT-b32 model and 86.58% for the ViT-l32 model. Furthermore, we have shown an end-to-end deployment of our proposed method on the NVIDIA Jetson Orin Nano board to demonstrate its practical use case in modern hand-held devices that are usually resource-constrained.",
      "url": "https://www.semanticscholar.org/paper/eba519c9e4e03fc4f52f3120a0d5803c62e36c4d",
      "embedding": null
    },
    {
      "cand_id": "bb775330f90da806a0c30758ea8b2babd48076ea",
      "title": "Improving the Adversarial Transferability of Vision Transformers with Virtual Dense Connection",
      "year": 2024,
      "venue": "AAAI Conference on Artificial Intelligence",
      "abstract": "With the great achievement of vision transformers (ViTs), transformer-based approaches have become the new paradigm for solving various computer vision tasks. However, recent research shows that similar to convolutional neural networks (CNNs), ViTs are still vulnerable to adversarial attacks. To explore the shared deficiency of models with different structures, researchers begin to analyze the cross-structure adversarial transferability, which is still under-explored. Therefore, in this work, we focus on the ViT attacks to improve the cross-structure transferability between the transformer-based and convolution-based models. Previous studies fail to thoroughly investigate the influence of the components inside the ViT models on adversarial transferability, leading to inferior performance. To overcome the drawback, we launch a motivating study by linearly down-scaling the gradients of components inside the ViT models to analyze their influence on adversarial transferability. Based on the motivating study, we find that the gradient of the skip connection most influences transferability and believe that back-propagating gradients from deeper blocks can enhance transferability. Therefore, we propose the Virtual Dense Connection method (VDC). Specifically, without changing the forward pass, we first recompose the original network to add virtual dense connections. Then we back-propagate gradients of deeper Attention maps and Multi-layer Perceptron (MLP) blocks via virtual dense connections when generating adversarial samples. Extensive experiments confirm the superiority of our proposed method over the state-of-the-art baselines, with an 8.2% improvement in transferability between ViT models and a 7.2% improvement in cross-structure transferability from ViTs to CNNs.",
      "url": "https://www.semanticscholar.org/paper/bb775330f90da806a0c30758ea8b2babd48076ea",
      "embedding": null
    }
  ],
  "stats": {
    "total_candidates": 20,
    "after_dedup": 17,
    "after_nontechnical_filter": 17,
    "after_year_filter": 17,
    "final": 17
  }
}